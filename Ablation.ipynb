{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ablation.ipynb","provenance":[{"file_id":"1GT-CD-z20ohShrVUyNtfYT552p7cn_QT","timestamp":1657225244226}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t0PSJpwN0Qcw","executionInfo":{"status":"ok","timestamp":1657046749665,"user_tz":240,"elapsed":17194,"user":{"displayName":"Muskan Garg","userId":"05698644322950347796"}},"outputId":"75edff19-5a00-454b-b4e7-4cb597e9bb9c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":[""],"metadata":{"id":"H4QpMUbS9DBp"}},{"cell_type":"code","source":["import networkx as nx\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","import string\n","import re\n","from nltk.stem import WordNetLemmatizer\n","SET_WINDOW_LENGTH=2\n","import matplotlib.pyplot as plt\n","from nltk.tokenize import WordPunctTokenizer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3kr0zl2VL9p_","executionInfo":{"status":"ok","timestamp":1657046754549,"user_tz":240,"elapsed":4889,"user":{"displayName":"Muskan Garg","userId":"05698644322950347796"}},"outputId":"e0e9c40f-c4b8-4d52-c809-bba6216a75f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","df1=pd.read_csv('drive/MyDrive/Chandni Muskan/Explainability for CAMS/CNN_LSTM_intepretationIG.csv', usecols= ['top_tokens','Interpretations','label'])\n","print(df1)\n","# Reset index after drop\n","df=df1.dropna().reset_index(drop=True)\n","print(df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9j3sjJ9b17Di","executionInfo":{"status":"ok","timestamp":1657046756125,"user_tz":240,"elapsed":1580,"user":{"displayName":"Muskan Garg","userId":"05698644322950347796"}},"outputId":"216e0beb-53db-44fe-8dec-386b7475b0dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                       Interpretations  \\\n","0    not productive, worthless, hate, bored , issue...   \n","1    feeling trapped, paranoid, barely talks to me,...   \n","2                                                 ugly   \n","3    abuse suffered, hard person to love, scars, cy...   \n","4    wasting money, sulking, failure, disappointmen...   \n","..                                                 ...   \n","365                                                NaN   \n","366  learn, sad, assure, worse, logic, leave, const...   \n","367                  gotten worse, tried so hard, pain   \n","368  anxiety, friends are disappearing, hit really ...   \n","369  ending the relationship, donât speak to my m...   \n","\n","                                            top_tokens  label  \n","0    ['constantly', 'everyday', 'looking', 'therapi...      1  \n","1    ['hours', 'everyday', 'hes', 'wrong', 'took', ...      5  \n","2            ['health', 'believe', None, 'im', 'dont']      1  \n","3    ['wouldnt', 'us', 'gets', 'wants', 'taking', '...      1  \n","4    ['might', 'change', 'part', 'kind', 'gone', 'g...      2  \n","..                                                 ...    ...  \n","365                ['says', '4', 'social', None, 'im']      0  \n","366  ['mean', 'please', 'saying', 'kind', 'therapy'...      5  \n","367           ['yet', 'came', 'read', 'recently', '4']      5  \n","368  ['different', 'part', 'sick', 'wanna', 'wrong'...      4  \n","369  ['alive', 'came', 'might', 'saying', 'mean', '...      4  \n","\n","[370 rows x 3 columns]\n","                                       Interpretations  \\\n","0    not productive, worthless, hate, bored , issue...   \n","1    feeling trapped, paranoid, barely talks to me,...   \n","2                                                 ugly   \n","3    abuse suffered, hard person to love, scars, cy...   \n","4    wasting money, sulking, failure, disappointmen...   \n","..                                                 ...   \n","336  try, convince, excuse, bad, deserve, injustice...   \n","337  learn, sad, assure, worse, logic, leave, const...   \n","338                  gotten worse, tried so hard, pain   \n","339  anxiety, friends are disappearing, hit really ...   \n","340  ending the relationship, donât speak to my m...   \n","\n","                                            top_tokens  label  \n","0    ['constantly', 'everyday', 'looking', 'therapi...      1  \n","1    ['hours', 'everyday', 'hes', 'wrong', 'took', ...      5  \n","2            ['health', 'believe', None, 'im', 'dont']      1  \n","3    ['wouldnt', 'us', 'gets', 'wants', 'taking', '...      1  \n","4    ['might', 'change', 'part', 'kind', 'gone', 'g...      2  \n","..                                                 ...    ...  \n","336  ['longer', '4', 'change', 'health', 'looking',...      1  \n","337  ['mean', 'please', 'saying', 'kind', 'therapy'...      5  \n","338           ['yet', 'came', 'read', 'recently', '4']      5  \n","339  ['different', 'part', 'sick', 'wanna', 'wrong'...      4  \n","340  ['alive', 'came', 'might', 'saying', 'mean', '...      4  \n","\n","[341 rows x 3 columns]\n"]}]},{"cell_type":"code","source":["#PRE-PROCESSING\n","def remove_punctuation(test_str):\n","  test_str = test_str.translate(str.maketrans(' ', ' ', string.punctuation))\n","  return test_str\n","def remove_stopwords(list_of_words):\n","  stop_words = set(stopwords.words('english'))\n","  texts=[t for t in list_of_words if t not in stop_words]\n","  return texts\n","def lemmatize(texts):\n","  lemmatizer = WordNetLemmatizer()\n","  new_texts=[]\n","  for each_word in texts:\n","    new_texts.append(lemmatizer.lemmatize(each_word))\n","  return new_texts\n","def remove_noise(texts):\n","  new_texts=[]\n","  for each_word in texts:\n","    if len(each_word)>1:\n","      new_texts.append(each_word )\n","  return new_texts\n","def preprocess(texts):\n","  texts=remove_punctuation(texts)\n","  texts=texts.lower()\n","  texts=WordPunctTokenizer().tokenize(texts)\n","  texts=remove_stopwords(texts)\n","  texts=lemmatize(texts)\n","  texts=remove_noise(texts)\n","  texts=' '.join(texts)\n","  return texts\n"],"metadata":{"id":"Dt0a0xygLz6O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","from nltk.tokenize import word_tokenize"],"metadata":{"id":"FMctvlqmHKvq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def taggeddata(data):\n","  tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]"],"metadata":{"id":"6H0FvVM2Sk3c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gensim\n","from gensim.models import Word2Vec\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","tokenized_doc=[]\n","tokenized_doc=list(df['top_tokens'])\n","\n","\n","tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_doc)]\n","\n","model1 = Doc2Vec(tagged_data, size = 100, window = 100, min_count = 1, workers = 4)\n","#model1 = Word2Vec(tagged_data,  size = 100, window = 5, min_count = 1)\n","\n","tokenized_doc=[]\n","for i in range(0, len(list(df['Interpretations']))):\n","  values=df['Interpretations'][i]\n","  values=str(values).split(',')\n","  tokenized_doc.append(values)\n","\n","tagged_data1 = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_doc)]\n","\n","model2 = Doc2Vec(tagged_data1, size = 100, window = 100, min_count = 1, workers = 4)\n","#model2 = Word2Vec(tagged_data, size = 100, window = 5, min_count = 1)\n","#model = Doc2Vec(tagged_data, size = 100, window = 300, min_count = 1, workers = 4)\n","\n","#! pip install testfixtures\n","#from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n","#new_model = ConcatenatedDoc2Vec((model1, model2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U7Cn3nHwdgtx","executionInfo":{"status":"ok","timestamp":1657046758900,"user_tz":240,"elapsed":1751,"user":{"displayName":"Muskan Garg","userId":"05698644322950347796"}},"outputId":"b39b060d-90a0-4c60-ad28-bceb78aa3851"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gensim/models/doc2vec.py:570: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n","  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from scipy.spatial.distance import cosine\n","def cosine_similarity(vector_1: np.array, vector_2: np.array) -> float:\n","    \"\"\"\n","    :param vector_1: vector of the first document\n","    :param vector_2: vector of the second document\n","    :return: cosine distance between the two documents\n","    \"\"\"\n","\n","    # calculate the cosine distance between two vectors\n","    if np.all((vector_1 == 0)) or np.all((vector_2 == 0)):\n","      return 0\n","\n","    # return 1 - cosine_distance to indicate similarity\n","    cosine_distance = cosine(vector_1, vector_2)\n","    return 1 - cosine_distance"],"metadata":{"id":"mL_K869KhRE5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import numpy as np\n","# ! pip install rouge-score\n","# from rouge_score import rouge_scorer"],"metadata":{"id":"x3Vh_vJzhxwP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !pip install textdistance\n","# !pip install \"textdistance[benchmark]\"\n","# !pip install \"textdistance[extras]\""],"metadata":{"id":"iucxl_he0fnI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gensim\n","from scipy import spatial\n","import gensim.downloader as api\n","\n","model = api.load(\"glove-wiki-gigaword-50\") #choose from multiple models https://github.com/RaRe-Technologies/gensim-data\n","calculate=0\n","\n","def preprocess(s):\n","    return [i.lower() for i in s.split()]\n","\n","def get_vector(s):\n","    return np.sum(np.array([model[i] for i in preprocess(s)]), axis=0)"],"metadata":{"id":"Euz4CKWAy4LW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657046808217,"user_tz":240,"elapsed":49320,"user":{"displayName":"Muskan Garg","userId":"05698644322950347796"}},"outputId":"d96cc660-f32c-49a9-dad7-cf96391a3801"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[==================================================] 100.0% 66.0/66.0MB downloaded\n"]}]},{"cell_type":"markdown","source":["# Results"],"metadata":{"id":"ocQPOYo_yl8z"}},{"cell_type":"code","source":["# import textdistance\n","# #model= Doc2Vec.load(\"d2v.model\")\n","# text1=[]\n","# text2=[]\n","# text0=[]\n","# calculate=0\n","# rouge=0\n","# scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n","\n","# hamming=0\n","# word2vec=0\n","# for each_row in range(0, len(df.index)):\n","#   doc1 = df['top_tokens'][each_row]\n","#   doc1=doc1.lstrip('[').rstrip(']')\n","#   doc1=doc1.split(',')\n","#   text_doc=''\n","#   for eachword in doc1:\n","#     eachword=eachword.strip()\n","#     eachword=eachword.lstrip(\"'\").rstrip(\"'\")\n","    \n","#     text_doc=text_doc+ eachword+' '\n","# #  doc1=''.join(lst_doc1)\n","# #  print(type(doc1))\n","  \n","#   text_doc=preprocess(text_doc)\n","#   text1.append(text_doc)\n","#   doc2=df['Interpretations'][each_row]\n","\n","\n","\n","#   vector1 = model1.infer_vector(text_doc)\n","#   vector2 = model2.infer_vector(str(doc2))\n","\n","#   cosine_sim=cosine_similarity(vector1, vector2)\n","#   calculate=calculate+ abs(cosine_sim)\n","\n","#   cosine_sim1=model.wmdistance(text_doc, str(doc2))\n","#   print(cosine_sim1)\n","#   word2vec=word2vec + abs(cosine_sim1)\n","\n","#   scores = scorer.score(str(text_doc), str(doc2))\n","#   print(scores)\n","#   #rouge=rouge+scores\n","\n","\n","#   hamming=hamming + textdistance.hamming.distance(text_doc, str(doc2))\n","\n","# print(calculate/370.0) #Doc2vec\n","# print(hamming/370.0) # Hamming\n","# print(word2vec/370.0) # word2vec\n","# print(rouge/370.0) # word2vec\n","# print(word2vec)"],"metadata":{"id":"PFDG5XIC5SOP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1. Using pretrained word2vec\n","\n","---\n","\n"],"metadata":{"id":"Ombe-6cf9F3_"}},{"cell_type":"code","source":["def get_fun(word):\n","  try:\n","    return get_vector(word)\n","  except:\n","    return 4\n"],"metadata":{"id":"u0DJyiVtl5ZF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import math\n","\n","calculate=0\n","flag=0\n","dist0=dist1=dist2=dist3=dist4=dist5=0.0\n","count0=count1=count2=count3=count4=count5=0\n","text1=[]\n","for each_row in range(0, len(df.index)):\n","  doc1 = df['top_tokens'][each_row]\n","  doc1=doc1.lstrip('[').rstrip(']')\n","  doc1=doc1.split(',')\n","  text_doc=''\n","  for eachword in doc1:\n","    eachword=eachword.strip()\n","    eachword=eachword.lstrip(\"'\").rstrip(\"'\")\n","    \n","    text_doc=text_doc+ eachword+' '\n","  \n","#  doc1=''.join(lst_doc1)\n","#  print(type(doc1))\n","  \n","  text_doc=preprocess(text_doc)\n","  text1.append(text_doc)\n","\n","  doc2=df['Interpretations'][each_row]\n","\n","  test=str(doc2)\n","  doc2=preprocess(str(doc2))\n","\n","\n","  text_doc2=[]\n","  for eachword in doc2:\n","\n","#   eachword=eachword.lstrip(\" \").rstrip(\" \")\n","    eachword=eachword.replace(\",\",\"\")\n","\n","    if not eachword.isdigit():\n","      text_doc2.append(str(eachword))\n","#  doc2=''.join(doc2)\n","  abc=[]\n","  vector1 = np.mean([get_fun(word) for word in text_doc], axis=0)\n","  for word in text_doc2:\n","    get=get_fun(word)\n","\n","    if get is not 4:\n","      abc.append(get)\n","\n","  vector2 = np.mean(abc, axis=0)\n","\n","  if(test!='nan'):\n","  #  print(\"NAN\")\n","# else:\n","    cosine_sim=cosine_similarity(vector1, vector2)\n","    x=abs(cosine_sim)\n","    if (math.isnan(cosine_sim)):\n","      calculate=calculate+ abs(cosine_sim)\n","      flag=flag+1\n","      x=0.0\n","\n"," #   x=abs(cosine_sim)\n","\n","    if(df['label'][each_row])==0:\n","      dist0=dist0+x\n","      count0=count0+1\n","    elif (df['label'][each_row])==1:\n","      dist1=dist1+x\n","      count1=count1+1\n","    elif (df['label'][each_row])==2:\n","      dist2=dist2+x\n","      count2=count2+1\n","    elif (df['label'][each_row])==3: \n","      dist3=dist3+x\n","      count3=count3+1\n","    elif (df['label'][each_row])==4: \n","      dist4=dist4+x\n","      count4=count4+1\n","    else:\n","      dist5=dist5+x\n","      count5=count5+1\n","print(calculate/(float)(flag))\n","print(\"Cosine between interpretations and top keywords for class 0 is \", dist0/count0)  \n","print(\"Cosine between interpretations and top keywords for class 1 is \", dist1/count1)  \n","print(\"Cosine between interpretations and top keywords for class 2 is \", dist2/count2) \n","print(\"Cosine between interpretations and top keywords for class 3 is \", dist3/count3)\n","print(\"Cosine between interpretations and top keywords for class 4 is \", dist4/count4)  \n","print(\"Cosine between interpretations and top keywords for class 5 is \", dist5/count5)  \n"],"metadata":{"id":"otKCcBZ9M9Rx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657046808405,"user_tz":240,"elapsed":191,"user":{"displayName":"Muskan Garg","userId":"05698644322950347796"}},"outputId":"6c54a975-d8f0-4555-eef2-40167ce68e90"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["nan\n","Cosine between interpretations and top keywords for class 0 is  0.7283725556917489\n","Cosine between interpretations and top keywords for class 1 is  0.7893811890057155\n","Cosine between interpretations and top keywords for class 2 is  0.851162555317084\n","Cosine between interpretations and top keywords for class 3 is  0.6900965016941691\n","Cosine between interpretations and top keywords for class 4 is  0.8702237370249989\n","Cosine between interpretations and top keywords for class 5 is  0.8152743490493816\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:163: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  arr = asanyarray(a)\n","/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n","  out=out, **kwargs)\n","/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n","  ret = ret.dtype.type(ret / rcount)\n"]}]},{"cell_type":"code","source":["\n","df2=df.groupby('label')\n","for eachdf in df2:\n","    calculate=0\n","    flag=0\n","    text1=[]\n","    eachdf=eachdf.reset_index()\n","    for each_row in range(0, len(eachdf)):\n","      doc1 = eachdf['top_tokens'][each_row]\n","      doc1=doc1.lstrip('[').rstrip(']')\n","      doc1=doc1.split(',')\n","      text_doc=''\n","      for eachword in doc1:\n","        eachword=eachword.strip()\n","        eachword=eachword.lstrip(\"'\").rstrip(\"'\")\n","        \n","        text_doc=text_doc+ eachword+' '\n","      \n","    #  doc1=''.join(lst_doc1)\n","    #  print(type(doc1))\n","      \n","      text_doc=preprocess(text_doc)\n","      text1.append(text_doc)\n","\n","      doc2=eachdf['Interpretations'][each_row]\n","\n","      test=str(doc2)\n","      doc2=preprocess(str(doc2))\n","    \n","    \n","      text_doc2=[]\n","      for eachword in doc2:\n","\n","    #   eachword=eachword.lstrip(\" \").rstrip(\" \")\n","        eachword=eachword.replace(\",\",\"\")\n","\n","        if not eachword.isdigit():\n","          text_doc2.append(str(eachword))\n","    #  doc2=''.join(doc2)\n","      abc=[]\n","      vector1 = np.mean([get_fun(word) for word in text_doc], axis=0)\n","      for word in text_doc2:\n","        get=get_fun(word)\n","    \n","        if get is not 4:\n","          abc.append(get)\n","\n","      vector2 = np.mean(abc, axis=0)\n","\n","      if(test!='nan'):\n","      #  print(\"NAN\")\n","    # else:\n","        cosine_sim=cosine_similarity(vector1, vector2)\n","        if (str(cosine_sim)!='nan'):\n","          calculate=calculate+ abs(cosine_sim)\n","          flag=flag+1\n","    print(calculate/(float)(flag))"],"metadata":{"id":"0V2LiMa89Sg9","colab":{"base_uri":"https://localhost:8080/","height":232},"executionInfo":{"status":"error","timestamp":1657046808928,"user_tz":240,"elapsed":525,"user":{"displayName":"Muskan Garg","userId":"05698644322950347796"}},"outputId":"cc792f30-9d92-4eb2-fe77-8231276f0fc8"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-bfc056545ef5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mflag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtext1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0meachdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meachdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0meach_row\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meachdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mdoc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meachdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'top_tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meach_row\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'reset_index'"]}]},{"cell_type":"code","source":["print(calculate/(float)(flag))\n","print(flag)"],"metadata":{"id":"sIhecLR7ujAC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gensim.downloader as api\n","\n","# Load the pre-trained model\n","model = api.load('fasttext-wiki-news-subwords-300')\n","calculate=0\n","\n","def preprocess(s):\n","    return [i.lower() for i in s.split()]\n","\n","def get_vector(s):\n","    return np.sum(np.array([model[i] for i in preprocess(s)]), axis=0)"],"metadata":{"id":"htANYBb22PXf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","text1=[]\n","flag=0\n","\n","#df_list=df.groupby(['label'])\n","#print(df_list)\n","df2=df.groupby('label')\n","for eachdf in df2:\n","    calculate=0\n","    flag=0\n","    text1=[]\n","    for each_row in range(0, len(eachdf)):\n","      doc1 = df['top_tokens'][each_row]\n","      doc1=doc1.lstrip('[').rstrip(']')\n","      doc1=doc1.split(',')\n","      text_doc=''\n","      for eachword in doc1:\n","        eachword=eachword.strip()\n","        eachword=eachword.lstrip(\"'\").rstrip(\"'\")\n","        \n","        text_doc=text_doc+ eachword+' '\n","      \n","    #  doc1=''.join(lst_doc1)\n","    #  print(type(doc1))\n","      \n","      text_doc=preprocess(text_doc)\n","      text1.append(text_doc)\n","\n","      doc2=df['Interpretations'][each_row]\n","\n","      test=str(doc2)\n","      doc2=preprocess(str(doc2))\n","    \n","    \n","      text_doc2=[]\n","      for eachword in doc2:\n","\n","    #   eachword=eachword.lstrip(\" \").rstrip(\" \")\n","        eachword=eachword.replace(\",\",\"\")\n","\n","        if not eachword.isdigit():\n","          text_doc2.append(str(eachword))\n","    #  doc2=''.join(doc2)\n","      abc=[]\n","      vector1 = np.mean([get_fun(word) for word in text_doc], axis=0)\n","      vector1=vector1.reshape(1,-1)\n","      for word in text_doc2:\n","        get=get_fun(word)\n","    \n","        if get is not 4:\n","          abc.append(get)\n","\n","      vector2 = np.mean(abc, axis=0)\n","      vector2=vector2.reshape(1,-1)\n","      print(vector1.shape)\n","      print(vector2.shape)\n","      cosine_sim=cosine_similarity(vector1, vector2)\n","      if (str(cosine_sim)!='nan'):\n","          calculate=calculate+ abs(cosine_sim)\n","          flag=flag+1\n","    print(calculate/(float)(flag))"],"metadata":{"id":"YQVDzmqNvGlD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#GUNJAN"],"metadata":{"id":"OVjEfc5i0wsA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.Jaccard Similarity\n","\n","---\n","\n"],"metadata":{"id":"NE78rF3EFq9E"}},{"cell_type":"code","source":["#!pip install textdistance\n","#!pip install \"textdistance[benchmark]\"\n","#!pip install \"textdistance[extras]\"\n"],"metadata":{"id":"GsP2MR2NFrxK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import textdistance\n","\n","# print('Hamming distance1',textdistance.hamming('test', 'text'))\n","\n","\n","# print ('Hamming distance2',textdistance.hamming.distance('test', 'text'))\n","\n","\n","# print ('Hamming distance3',textdistance.hamming.similarity('test', 'text'))\n","\n","\n","# print ('Hamming distance4',textdistance.hamming.normalized_distance('test', 'text'))\n","\n","\n","# print ('Hamming distance5',textdistance.hamming.normalized_similarity('test', 'text'))\n","\n","\n","# print ('Hamming distance6',textdistance.Hamming(qval=2).distance('test', 'text'))\n"],"metadata":{"id":"lZBdEgflGAGW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import textdistance\n","# from sklearn.metrics.pairwise import cosine_similarity\n","# import spacy\n","# nlp = spacy.load(\"en_core_web_sm\")\n","\n","# def text_processing(sentence):\n","#     \"\"\"\n","#     Lemmatize, lowercase, remove numbers and stop words\n","    \n","#     Args:\n","#       sentence: The sentence we want to process.\n","    \n","#     Returns:\n","#       A list of processed words\n","#     \"\"\"\n","#     sentence = [token.lemma_.lower()\n","#                 for token in nlp(sentence) \n","#                 if token.is_alpha and not token.is_stop]\n","    \n","#     return sentence\n","\n","# def jaccard_sim(row):\n","#     # Text Processing\n","#     sentence1 = text_processing(row['sentence1'])\n","#     sentence2 = text_processing(row['sentence2'])\n","    \n","#     # Jaccard similarity\n","#     return textdistance.jaccard.normalized_similarity(sentence1, sentence2)\n","\n","\n","\n","\n"],"metadata":{"id":"4EpWc0iIG9CW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Word Mover Distance\n","\n","---\n","\n"],"metadata":{"id":"wTdhPRtWXIP4"}},{"cell_type":"code","source":["# import gensim.downloader as api\n","\n","# # Load the pre-trained model\n","# model = api.load('fasttext-wiki-news-subwords-300')\n","\n","# def word_movers_distance(row):\n","#     # Text Processing\n","#     sentence1 = text_processing(row['sentence1'])\n","#     sentence2 = text_processing(row['sentence2'])\n","    \n","#     # Negative Word Movers Distance\n","#     return model.wmdistance(sentence1, sentence2)\n"],"metadata":{"id":"wlRqhbqAW2Fz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["word_movers_distance(row)"],"metadata":{"id":"n9gZxGdwXQGR"},"execution_count":null,"outputs":[]}]}